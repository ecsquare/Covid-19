{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMMwithGibsSampling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEZpNdjHH/+TjW6x2xGJ5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ecsquare/Covid-19/blob/master/DMMwithGibsSampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6plGEeQqwqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJZIgRC3rC2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "091a7805-4dc1-45a4-9b3b-74a0d6e49611"
      },
      "source": [
        "cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/vegan_tweets_2020-05-/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/vegan_tweets_2020-05-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXS-QdiorMgH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "b79a9130-7b29-41fa-ee49-9e1f3e7a2a75"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " corpus.txt\t\t'vegan_2020-05-02 2.json'   vegan_2020-05-06.json\n",
            " online_btm.html\t vegan_2020-05-03.json\t   'vegan_2020-05-07 2.json'\n",
            " sample_data\t\t vegan_2020-05-04.json\n",
            " vegan_2020-05-01.json\t vegan_2020-05-05.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhZrecVFrOnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "data = [json.loads(line)['tweet'] for line in open('vegan_2020-05-01.json', 'r')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9HmPtz__Y2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "78cdb797-368b-49bc-fa98-62fd594e6e24"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJa6z4gLrYBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def clean_tweets(text):\n",
        "  text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "  text = re.sub('[0-9]+', '', text)\n",
        "\n",
        "  #remove stopwords\n",
        "  stop_words = set(stopwords.words('english')) \n",
        "  word_tokens = word_tokenize(text) \n",
        "  filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "  text  = \" \".join(filtered_sentence)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH8wumzRrd-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [clean_tweets(k) for k in data]\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xR0pgM5TZJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('corpus.txt', 'w') as f:\n",
        "    for i in range(50):\n",
        "        f.write(\"%s\\n\" % data[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFzZ426KL6YT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "class GibbsSamplingDMM(object):\n",
        "\t\n",
        "\tnumDocuments = 0\n",
        "\tnumWordsInCorpus = 0\n",
        "\tword2IdVocabulary = {}\n",
        "\tid2WordVocabulary = {}\n",
        "\tdocuments = []\n",
        "\toccurenceToIndexCount = []\n",
        "\ttopicAssignments = []\n",
        "\tdocTopicCount = []\n",
        "\ttopicWordCount = []\n",
        "\tsumTopicWordCount = []\n",
        "\tmultiPros = []\n",
        "\tbetaSum=0.\n",
        "\n",
        "\tdef __init__(self, paramters):\n",
        "\t\tsuper(GibbsSamplingDMM, self).__init__()\n",
        "\t\tself.corpus = paramters[\"corpus\"]\n",
        "\t\tself.output = paramters[\"output\"]\n",
        "\t\tself.ntopics = int(paramters[\"ntopics\"])\n",
        "\t\tself.alpha = float(paramters[\"alpha\"])\n",
        "\t\tself.beta = float(paramters[\"beta\"])\n",
        "\t\tself.niters = int(paramters[\"niters\"])\n",
        "\t\tself.twords = int(paramters[\"twords\"])\n",
        "\t\tself.name = paramters[\"name\"]\n",
        "\n",
        "\tdef analyseCorpus(self):\n",
        "\t\tindexWord=0\n",
        "\t\tdata = open(self.corpus,'r')\n",
        "\t\tfor doc in data:\n",
        "\t\t\tdocument = []\n",
        "\t\t\twordOccurenceToIndexInDocCount = {}\n",
        "\t\t\twordOccurenceToIndexInDoc = []\n",
        "\t\t\tif doc.rstrip!=None:\n",
        "\t\t\t\twords = doc.rstrip().split()\n",
        "\t\t\t\tfor word in words:\n",
        "\n",
        "\t\t\t\t\tif word in self.word2IdVocabulary:\n",
        "\t\t\t\t\t\tdocument.append(self.word2IdVocabulary[word])\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tself.word2IdVocabulary[word]=indexWord\n",
        "\t\t\t\t\t\tself.id2WordVocabulary[indexWord]=word\n",
        "\t\t\t\t\t\tdocument.append(indexWord)\n",
        "\t\t\t\t\t\tindexWord+=1\n",
        "\n",
        "\t\t\t\t\tif word in wordOccurenceToIndexInDocCount:\n",
        "\t\t\t\t\t\twordOccurenceToIndexInDocCount[word]+=1\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\twordOccurenceToIndexInDocCount[word]=1\n",
        "\n",
        "\t\t\t\t\twordOccurenceToIndexInDoc.append(wordOccurenceToIndexInDocCount[word])\n",
        "\n",
        "\t\t\t\tself.numWordsInCorpus+=len(document)\n",
        "\t\t\t\tself.numDocuments+=1\n",
        "\t\t\t\tself.documents.append(document)\n",
        "\t\t\t\tself.occurenceToIndexCount.append(wordOccurenceToIndexInDoc)\n",
        "\n",
        "\t\tself.betaSum=len(self.word2IdVocabulary)*self.beta\n",
        "\n",
        "\n",
        "\tdef topicAssigmentInitialise(self):\n",
        "\t\tself.docTopicCount = [0 for x in range(self.ntopics)]\n",
        "\t\tself.sumTopicWordCount = [0 for x in range(self.ntopics)]\n",
        "\n",
        "\t\tfor i in range(self.ntopics):\n",
        "\t\t\t\tself.topicWordCount.append([0 for x in range(len(self.word2IdVocabulary))])\n",
        "\n",
        "\t\tfor i in range (self.numDocuments):\n",
        "\t\t\ttopic = random.randint(0,self.ntopics-1)\n",
        "\t\t\tself.docTopicCount[topic]+=1\n",
        "\t\t\t\n",
        "\t\t\tfor j in range (len(self.documents[i])):\n",
        "\t\t\t\tself.topicWordCount[topic][self.documents[i][j]]+=1\n",
        "\t\t\t\tself.sumTopicWordCount[topic]+=1\n",
        "\n",
        "\t\t\tself.topicAssignments.append(topic)\n",
        "\n",
        "\tdef nextDiscrete(self,a):\n",
        "\t\tb = 0.\n",
        "\n",
        "\t\tfor i in range(len(a)):\n",
        "\t\t\tb+=a[i]\n",
        "\n",
        "\t\tr = random.uniform(0.,1.)*b\n",
        "\t\t\n",
        "\t\tb=0.\n",
        "\t\tfor i in range (len(a)):\n",
        "\t\t\tb+=a[i]\n",
        "\t\t\tif(b>r):\n",
        "\t\t\t\treturn i\n",
        "\t\treturn len(a)-1\n",
        "\n",
        "\tdef sampleInSingleIteration(self,x):\n",
        "\t\tprint (\"iteration: \"+str(x))\n",
        "\t\tfor d in range(self.numDocuments):\n",
        "\t\t\ttopic = self.topicAssignments[d]\n",
        "\t\t\tself.docTopicCount[topic]-=1\n",
        "\t\t\tdocSize = len(self.documents[d])\n",
        "\t\t\tdocument = self.documents[d]\n",
        "\n",
        "\t\t\tfor w in range(docSize):\n",
        "\t\t\t\tword = document[w]\n",
        "\t\t\t\tself.topicWordCount[topic][word]-=1\n",
        "\t\t\t\tself.sumTopicWordCount[topic]-=1\n",
        "\t\t\t\n",
        "\n",
        "\t\t\tfor t in range(self.ntopics):\n",
        "\t\t\t\tself.multiPros[t] = self.docTopicCount[t]+self.alpha\n",
        "\n",
        "\t\t\t\tfor w in range(docSize):\n",
        "\t\t\t\t\tword = document[w]\n",
        "\t\t\t\t\tself.multiPros[t] *= (self.topicWordCount[t][word]+self.beta+self.occurenceToIndexCount[d][w]-1)/(self.sumTopicWordCount[t]+w+self.betaSum)\n",
        "\t\t\t\t\n",
        "\t\t\t\t\t\n",
        "\t\t\t#print self.multiPros\n",
        "\t\t\ttopic = self.nextDiscrete(self.multiPros)\n",
        "\t\t\t#print topic\n",
        "\n",
        "\t\t\tself.docTopicCount[topic]+=1\n",
        "\n",
        "\t\t\tfor w in range(docSize):\n",
        "\t\t\t\tword = document[w]\n",
        "\t\t\t\tself.topicWordCount[topic][word]+=1\n",
        "\t\t\t\tself.sumTopicWordCount[topic]+=1\n",
        "\n",
        "\t\t\tself.topicAssignments[d] = topic\n",
        "\n",
        "\tdef inference(self):\n",
        "\t\tself.multiPros = [0 for x in range(self.ntopics)]\n",
        "\t\t[self.sampleInSingleIteration(x) for x in range(self.niters)]\n",
        "\n",
        "\tdef writeTopicAssignments(self):\n",
        "\t\tfile = open(self.output+self.name+\"topicAssignments.txt\",\"w\")\n",
        "\t\t#for i in range(self.numDocuments):\n",
        "\t\t[file.write(str(self.topicAssignments[i])+\"\\n\") for i in range(self.numDocuments)]\n",
        "\n",
        "\n",
        "\n",
        "\tdef writeTopTopicalWords(self):\n",
        "\t\tfile = open(self.output+self.name+\"topWords.txt\",\"w\") \n",
        "\t\tfor t in range(self.ntopics):\n",
        "\t\t\twordCount = {w:self.topicWordCount[t][w] for w in range(len(self.word2IdVocabulary))}\n",
        "\t\t\t\n",
        "\t\t\tcount =0\n",
        "\t\t\tstring=\"Topic \"+str(t)+\": \"\n",
        "\t\t\t\n",
        "\t\t\tfor index in sorted(wordCount, key=wordCount.get, reverse=True):\n",
        "\t\t\t\tstring += self.id2WordVocabulary[index]+\" \"\n",
        "\t\t\t\tcount+=1\n",
        "\t\t\t\tif count>=self.twords:\n",
        "\t\t\t\t\tfile.write(string+\"\\n\") \n",
        "\t\t\t\t\t# print string\n",
        "\t\t\t\t\tbreak\n",
        "\t\tfile.close()\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii_4I3r3uOp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {\"corpus\":\"corpus.txt\", \n",
        "          \"output\": \"out\", \n",
        "          \"ntopics\": \"30\", \n",
        "          \"alpha\":\"0.1\",\n",
        "          \"beta\":\"0.1\", \n",
        "          \"niters\":\"300\",\n",
        "          \"twords\":\"30\",\n",
        "          \"name\":\"model\"}\n",
        "\n",
        "model = GibbsSamplingDMM(params)\n",
        "model.analyseCorpus()\n",
        "model.topicAssigmentInitialise()\n",
        "model.inference()\n",
        "\t\n",
        "print (\"Writing Results\")\n",
        "model.writeTopTopicalWords()\n",
        "model.writeTopicAssignments()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye-E3IDgTfia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65k8pXQ3ut8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}